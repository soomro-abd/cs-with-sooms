{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soomro-abd/cs-with-sooms/blob/main/group24_phase1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpn9k3BV1X2s"
      },
      "source": [
        "# CS331 - Spring 2022 - Phase 1 [10%]\n",
        "\n",
        "*__Submission Guidelines:__*\n",
        "- Naming convention for submission of this notebook is `groupXX_phase1.ipynb` where XX needs to be replaced by your group number. For example: group 1 would rename their notebook to `group01_phase1.ipynb`\n",
        "- Only the group lead is supposed to make the submission\n",
        "- All the cells <b>must</b> be run once before submission. If your submission's cells are not showing the results (plots etc.), marks wil be deducted\n",
        "- Only the code written within this notebook will be considered while grading. No other files will be entertained\n",
        "- You are advised to follow good programming practies including approriate variable naming and making use of logical comments \n",
        "\n",
        "Please note that your notebooks will be checked against submissions from last year's course offering for plagiarism. The university honor code should be maintained. Any violation, if found, will result in disciplinary action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9cjup_91X2_"
      },
      "source": [
        "#### <b>Introduction</b> \n",
        "This is the first of the three phases of this offering's project. To give an overview of this phase, we will essentially be building everything from scratch. The datasets that we will be using for this project are the MNIST and the Fashion_MNIST dataset. <b> This notebook will focus on the Fashion_MNIST dataset. </b> \n",
        "\n",
        "The Fashion_MNIST dataset consists of 70,000 images of fashion/clothing items belonging to 10 different categories/classes. It has further been divided into 60,000 training images and 10,000 test images and each image is a 28*28 grayscale image (hence 1 color channel). It is recommended that you go through [this link](https://www.kaggle.com/zalando-research/fashionmnist) to familiarize yourself with the dataset.\n",
        "\n",
        "You will begin by manually loading both the dataset in this notebook (more instructions on this will follow) followed by from-scratch implementation of a Neural Netowrk (NN). Once done, you will have to tweak the hyperparameters (such as learning rate, number of epochs etc.) to get the best results for your NN's implementation\n",
        "\n",
        "###### <b>You will strictly be using for-loops fort this phase's implementation of NN (unless specified otherwise in the sub-section)\n",
        "\n",
        "###### Modification of the provided code without prior discussion with the TAs will result in a grade deduction</b>\n",
        "\n",
        "---\n",
        "\n",
        "###### <b>Side note</b>\n",
        "The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/). If you do not wish to use this then simply comment out the import for `pydot`\n",
        "\n",
        "###### <b>Need Help?</b>\n",
        "If you need help, please refer to the course staff ASAP and do not wait till the last moment as they might not be available on very short notice close to deadlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5m1T5nJ6WH3"
      },
      "source": [
        "#### <b>Before You Begin</b>\n",
        "\n",
        "Skeleton code is provided to get you started. The main methods that you need to implement correspond to the four steps of the training process of a NN which are as follows:\n",
        "1. Initialize variables and initialize weights\n",
        "2. Forward pass\n",
        "3. Backward pass AKA Backpropagation\n",
        "4. Weight Update AKA Gradient Descent\n",
        "\n",
        "__Look for comments in the code to see where you are supposed to write your code__ \n",
        "\n",
        "A `fit` function is what combines the previous three functions and overall trains the network to __fit__ to the provided training examples. The provided `fit` methods requires all the four steps of the training process to be working correctly. The function has been setup in a way that it expects the above four methods to take particular inputs and return particular outputs. __You are supposed to work within this restriction__ \n",
        "\n",
        "\n",
        "\n",
        "__To see if your model is working correctly, you need to make sure that your model loss is going down during training__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-30T14:39:17.671980Z",
          "start_time": "2019-01-30T14:38:40.137027Z"
        },
        "id": "UPL8Ut1u1X2-"
      },
      "outputs": [],
      "source": [
        "# making all the necessary imports here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "from IPython.display import Image\n",
        "import pydot\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y-6i2TL1X3C"
      },
      "outputs": [],
      "source": [
        "# This function will be used to plot the confusion matrix at the end of this notebook\n",
        "\n",
        "def plot_confusion_matrix(conf_mat):\n",
        "    classes = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "    df_cm = pd.DataFrame(conf_mat,classes,classes)\n",
        "    plt.figure(figsize=(15,9))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})\n",
        "    plt.show()\n",
        "\n",
        "class_labels = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-30T14:36:37.383767Z",
          "start_time": "2019-01-30T14:36:37.333537Z"
        },
        "id": "xU_yqbc31X2_"
      },
      "outputs": [],
      "source": [
        "# Enter group lead's roll number here. This will be used for plotting purposes\n",
        "\n",
        "rollnumber = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHrz4X069_6D"
      },
      "source": [
        "#### __Read dataset__\n",
        "\n",
        "Get paths for all the training and test images in the dataset and print the length of training and test paths' list. For this purpose you can use glob. You can have a look [here](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/) on how to use glob. The dataset that has been provided to you guys is a truncated version of the Fashion MNIST dataset (having 2000 training images and 1600 test images, only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g1jPQ5xaTlj"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive here\n",
        "drive.mount('/drive')\n",
        "\n",
        "# Edit this address so that it points to the dataset's zipped file on your Google Drive\n",
        "!unzip -o -q \"/drive/MyDrive/dataset.zip\" -d \"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBj9_sg9-Auu"
      },
      "outputs": [],
      "source": [
        "classes = 10  # do not change this\n",
        "x_train = None  # you must store the training images in this variable \n",
        "y_train = None  # you must store the training images' labels in this variable\n",
        "x_test = None   # you must store the test images in this variable\n",
        "y_test = None   # you must store the test images' labels in this variable\n",
        "\n",
        "###### Code Here ######\n",
        "'''Please note that you will have to extract and one-hot encode the labels of the images for both y_train and y_test'''\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of training sample: \", len(x_train))  # You can change len(X_train) based on your implementation such that total number of training samples is printed\n",
        "print(\"Number of testing sample: \", len(x_test))    # You can change len(X_test) based on your implementation such that total number of test samples is printed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QOxoQ-_B1X3B"
      },
      "source": [
        "#### __NN Implementation__\n",
        "Your implementation of NN needs to use the `sigmoid` activation function for the hidden layer(s) and the `softmax` activation function for the output layer. The NN model you will be creating here will consits of only three layers: 1 input layer, 1 hidden layer and 1 output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-27T01:25:17.044301Z",
          "start_time": "2019-01-27T01:25:16.984346Z"
        },
        "hidden": true,
        "id": "iP5ZI2Rs1X3C"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "    \n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(y_pred, y_true):\n",
        "        ###### Code Here ######\n",
        "        \n",
        "  \n",
        "        return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def accuracy(y_pred, y_true):\n",
        "        ###### Code Here ######\n",
        "        \n",
        "\n",
        "        return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        ###### Code Here ######\n",
        "\n",
        "\n",
        "        return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        ###### Code Here ######\n",
        "\n",
        "\n",
        "        return None\n",
        "    \n",
        "    def __init__(self, input_size, hidden_nodes, output_size):\n",
        "        '''Creates a Feed-Forward Neural Network.\n",
        "        The parameters represent the number of nodes in each layer (total 3). \n",
        "        Look at the inputs to the function'''\n",
        "        \n",
        "        self.num_layers = 3\n",
        "        self.input_shape = input_size\n",
        "        self.hidden_shape = hidden_nodes\n",
        "        self.output_shape = output_size\n",
        "        \n",
        "        self.weights_ = []\n",
        "        self.biases_ = []\n",
        "        self.__init_weights()\n",
        "    \n",
        "    def __init_weights(self):\n",
        "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
        "        \n",
        "        ###### Code Here (Replace 'None' by appropriate values/variables) ######\n",
        "        \n",
        "        W_h = np.random.normal(size=(None,None))\n",
        "        b_h = np.zeros(shape=(None,))\n",
        "\n",
        "        W_o = np.random.normal(size=(None,None))\n",
        "        b_o = np.zeros(shape=(None,))\n",
        "        \n",
        "        # self.weights_ becomes a list of np.arrays. 0th index has W_h and 1st index has W_o\n",
        "        self.weights_.append(W_h)  \n",
        "        self.weights_.append(W_o)  \n",
        "\n",
        "        # self.biases_ becomes a list of np.arrays. 0th index has b_h and 1st index has b_o\n",
        "        self.biases_.append(b_h)\n",
        "        self.biases_.append(b_o)\n",
        "\n",
        "    def forward_pass(self, input_data):\n",
        "        '''Executes the feed forward algorithm.\n",
        "        \"input_data\" is the input to the network in row-major form\n",
        "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
        "\n",
        "        ###### Code Here ######\n",
        "        \n",
        "\n",
        "        return activations\n",
        "\n",
        "    def backward_pass(self, targets, layer_activations):\n",
        "        '''Executes the backpropogation algorithm.\n",
        "        \"targets\" is the ground truth/labels\n",
        "        \"layer_activations\" are the return value of the forward pass step\n",
        "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "        \n",
        "\n",
        "        return deltas\n",
        "    \n",
        "    def weight_update(self, deltas, layer_inputs, lr):\n",
        "        '''Executes the gradient descent algorithm.\n",
        "        \"deltas\" is return value of the backward pass step\n",
        "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
        "        \"lr\" is the learning rate'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    ###### Do Not Change Anything Below this line in This Cell ######\n",
        "    \n",
        "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
        "            history = []\n",
        "            for epoch in tqdm_notebook(range(epochs)):\n",
        "                num_samples = Xs.shape[0]\n",
        "                for i in range(num_samples):\n",
        "\n",
        "                    sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
        "                    sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
        "                    \n",
        "                    activations = self.forward_pass(sample_input)   # Call forward_pass function \n",
        "                    deltas = self.backward_pass(sample_target, activations)    # Call backward_pass function \n",
        "                    layer_inputs = [sample_input] + activations[:-1]\n",
        "                    \n",
        "                    # Call weight_update function \n",
        "                    self.weight_update(deltas, layer_inputs, lr)\n",
        "                \n",
        "                preds = self.predict(Xs)   # Call predict function \n",
        "\n",
        "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
        "                \n",
        "                if  epoch==epochs-1:\n",
        "                  confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1),labels=np.arange(10))  \n",
        "                  plot_confusion_matrix(confusion_mat)\n",
        "                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1),num_classes=classes), target_names=class_labels)\n",
        "                  print(report)\n",
        "                history.append(current_loss)\n",
        "            return history\n",
        "    \n",
        "    def predict(self, Xs):\n",
        "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
        "        predictions = []\n",
        "        num_samples = Xs.shape[0]\n",
        "        for i in range(num_samples):\n",
        "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
        "            sample_prediction = self.forward_pass(sample)[-1]\n",
        "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def evaluate(self, Xs, Ys):\n",
        "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
        "        pred = self.predict(Xs)\n",
        "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
        "    \n",
        "    def plot_model(self, filename):\n",
        "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
        "        graph = pydot.Dot(graph_type='digraph')\n",
        "        graph.set_rankdir('LR')\n",
        "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
        "        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]\n",
        "        for i in range(self.num_layers-1):\n",
        "            for n1 in range(nodes_per_layer[i]):\n",
        "                for n2 in range(nodes_per_layer[i+1]):\n",
        "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
        "                    graph.add_edge(edge)\n",
        "        graph.write_png(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_200Z0dwE2Fq"
      },
      "outputs": [],
      "source": [
        "# These are what we call the hyperparameters (a.k.a Black Magic). You need to research on them and tweak them to see what generates the best result for you \n",
        "\n",
        "INPUT_SIZE = None       # must be an int, this number represents the numeber of nodes/neurons in the input layer of the network\n",
        "HIDDEN_NODES = None     # must be an int, this number represents the numeber of nodes/neurons in the only hidden layer of the network\n",
        "OUTPUT_SIZE = None      # must be an int, this number represents the numeber of nodes/neurons in the output layer of the network\n",
        "EPOCH = None            # must be an int\n",
        "LEARNING_RATE = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRtc2Dzc2D_H"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "nn = NeuralNetwork(input_size = INPUT_SIZE, hidden_nodes = HIDDEN_NODES, output_size = OUTPUT_SIZE)\n",
        "history = nn.fit(x_train, y_train, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "plt.plot(history);\n",
        "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));\n",
        "end = time.time()\n",
        "\n",
        "print(\"Runtime of the algorithm is \", round((end - start),3),\" seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "group24_phase1_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "notify_time": "5",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}